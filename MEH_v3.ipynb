{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "e084e610-8128-4769-ab64-6aa194044892",
        "_uuid": "20c011dd401be7b6448c43f965e5d0bf548c53b9",
        "id": "6tb7pPWSr3DS"
      },
      "source": [
        "# Beginners Guide to Text Generation using LSTMs\n",
        "\n",
        "Text Generation is a type of Language Modelling problem. Language Modelling is the core problem for a number of of natural language processing tasks such as speech to text, conversational system, and text summarization. A trained language model learns the likelihood of occurrence of a word based on the previous sequence of words used in the text. Language models can be operated at character level, n-gram level, sentence level or even paragraph level. In this notebook, I will explain how to create a language model for generating natural language text by implement and training state-of-the-art Recurrent Neural Network. \n",
        "\n",
        "### Generating News headlines \n",
        "\n",
        "In this kernel, I will be using the dataset of [New York Times Comments and Headlines](https://www.kaggle.com/aashita/nyt-comments) to train a text generation language model which can be used to generate News Headlines\n",
        "\n",
        "\n",
        "## 1. Import the libraries\n",
        "\n",
        "As the first step, we need to import the required libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "ODSWC-KHr3DW",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# keras module for building LSTM \n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.models import Sequential\n",
        "import tensorflow.keras.utils as ku \n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string, os \n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "id": "6AeZDFlJr3DX"
      },
      "source": [
        "## 2. Load the dataset\n",
        "\n",
        "Load the dataset of news headlines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "_cell_guid": "b8ef1429-ff19-4a6c-92d7-af8cc61c55f7",
        "_uuid": "87836e3adbe046dd0db62013491ba62bae93b6be",
        "id": "ir-lGUKrr3DY",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(\"./inspiration.csv\", index_col = 0)\n",
        "data\n",
        "data[\"all_text\"] = data[\"Category\"].astype(str) +\" \"+ data[\"Quote\"]\n",
        "new_data = data[\"all_text\"]\n",
        "all_headlines = new_data.tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "9dbd8bc9-fb61-43b9-b0c4-98bd7f3f8150",
        "_uuid": "fda5d4868631d3618d4d9a9a863541b2faf121c0",
        "id": "NSsu0-Cwr3DY"
      },
      "source": [
        "## 3. Dataset preparation\n",
        "\n",
        "### 3.1 Dataset cleaning \n",
        "\n",
        "In dataset preparation step, we will first perform text cleaning of the data which includes removal of punctuations and lower casing all the words. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "_cell_guid": "b8bf84ed-da11-4f89-a584-9dceea677420",
        "_uuid": "2a07365a27a7ba2f92fc9ba4d05d8e6254a68d8c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yvROc4y0r3DY",
        "outputId": "c8e54807-768a-497d-8452-63b83a36ccaf",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['love let us see what love can do. ',\n",
              " 'love we cant heal the world today. but we can begin with a voice of compassion, a heart of love, and an act of kindness. ',\n",
              " 'listening listen with curiosity. speak with honesty. act with integrity. ',\n",
              " 'listening the most basic and powerful way to connect to another person is to listen. just listen. ',\n",
              " 'listening knowledge speaks, but wisdom listens. ',\n",
              " 'listening deep listening is the kind of listening that can help relieve the suffering of another person. ',\n",
              " 'listening every person in this life has something to teachand as soon as you accept that, you open yourself to truly listening. ',\n",
              " 'stewardship there are no problems we cannot solve together, and very few that we can solve by ourselves. ',\n",
              " 'stewardship i always wondered why somebody didnt do something about that; then i realized that i am somebody. ',\n",
              " 'stewardship let us develop respect for all living things. let us try to replace violence and intolerance with understanding, compassion and love. ']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "def clean_text(txt):\n",
        "    txt = \"\".join(v for v in txt).lower()\n",
        "    txt = txt.encode(\"utf8\").decode(\"ascii\",'ignore')\n",
        "    return txt \n",
        "\n",
        "corpus = [clean_text(x) for x in all_headlines]\n",
        "corpus[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "9d83cc08-19ba-4b00-9ca6-dcf5ff39c8af",
        "_uuid": "6fd11859fd71aa5c7ce10bdbbd31c8eb6d1b3118",
        "id": "kLqG9LEyr3DY"
      },
      "source": [
        "### 3.2 Generating Sequence of N-gram Tokens\n",
        "\n",
        "Language modelling requires a sequence input data, as given a sequence (of words/tokens) the aim is the predict next word/token.  \n",
        "\n",
        "The next step is Tokenization. Tokenization is a process of extracting tokens (terms / words) from a corpus. Pythonâ€™s library Keras has inbuilt model for tokenization which can be used to obtain the tokens and their index in the corpus. After this step, every text document in the dataset is converted into sequence of tokens. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "_cell_guid": "896543c9-7944-4748-b8ef-ef8cbc2a84f0",
        "_uuid": "9129a8b773feb72eff91aa0025157a173d10c625",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fxSbEQuBr3DZ",
        "outputId": "d9d8420e-cc92-47ad-ecfd-fdc9b3c9a898",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[27, 113],\n",
              " [27, 113, 57],\n",
              " [27, 113, 57, 126],\n",
              " [27, 113, 57, 126, 21],\n",
              " [27, 113, 57, 126, 21, 27],\n",
              " [27, 113, 57, 126, 21, 27, 20],\n",
              " [27, 113, 57, 126, 21, 27, 20, 24],\n",
              " [27, 113, 57, 126, 21, 27, 20, 24, 1],\n",
              " [27, 14],\n",
              " [27, 14, 389]]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "tokenizer = Tokenizer(filters='!\"#$%&()*+-/:;<=>@[\\\\]^_`{|}~\\t\\n',)\n",
        "\n",
        "def get_sequence_of_tokens(corpus):\n",
        "    ## tokenization\n",
        "    new_corpus = []\n",
        "    for x in corpus:\n",
        "      to_tokenize = ['.',',',':',';','!','?']\n",
        "      end_tokenize = [' .',' ,',' :',' ;',' !',' ?']\n",
        "      for i, v in enumerate(to_tokenize):\n",
        "        x = x.replace(v, end_tokenize[i])\n",
        "      new_corpus.append(x)\n",
        "    corpus = new_corpus\n",
        "    tokenizer.fit_on_texts(corpus)\n",
        "    total_words = len(tokenizer.word_index) + 1\n",
        "    \n",
        "    ## convert data to sequence of tokens \n",
        "    input_sequences = []\n",
        "    for line in corpus:\n",
        "        token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "        for i in range(1, len(token_list)):\n",
        "            n_gram_sequence = token_list[:i+1]\n",
        "            input_sequences.append(n_gram_sequence)\n",
        "    return input_sequences, total_words\n",
        "\n",
        "inp_sequences, total_words = get_sequence_of_tokens(corpus)\n",
        "inp_sequences[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "a22c88f5-f2a3-457c-835b-63341e657e3f",
        "_uuid": "f22aa5e0c04620ca5034ab9389322eee543060c6",
        "id": "7tZdFjG8r3DZ"
      },
      "source": [
        "In the above output [30, 507], [30, 507, 11], [30, 507, 11, 1] and so on represents the ngram phrases generated from the input data. where every integer corresponds to the index of a particular word in the complete vocabulary of words present in the text. For example\n",
        "\n",
        "**Headline:** i stand  with the shedevils  \n",
        "**Ngrams:** | **Sequence of Tokens**\n",
        "\n",
        "<table>\n",
        "<tr><td>Ngram </td><td> Sequence of Tokens</td></tr>\n",
        "<tr> <td>i stand </td><td> [30, 507] </td></tr>\n",
        "<tr> <td>i stand with </td><td> [30, 507, 11] </td></tr>\n",
        "<tr> <td>i stand with the </td><td> [30, 507, 11, 1] </td></tr>\n",
        "<tr> <td>i stand with the shedevils </td><td> [30, 507, 11, 1, 975] </td></tr>\n",
        "</table>\n",
        "\n",
        "\n",
        "\n",
        "### 3.3 Padding the Sequences and obtain Variables : Predictors and Target\n",
        "\n",
        "Now that we have generated a data-set which contains sequence of tokens, it is possible that different sequences have different lengths. Before starting training the model, we need to pad the sequences and make their lengths equal. We can use pad_sequence function of Kears for this purpose. To input this data into a learning model, we need to create predictors and label. We will create N-grams sequence as predictors and the next word of the N-gram as label. For example:\n",
        "\n",
        "\n",
        "Headline:  they are learning data science\n",
        "\n",
        "<table>\n",
        "<tr><td>PREDICTORS </td> <td>           LABEL </td></tr>\n",
        "<tr><td>they                   </td> <td>  are</td></tr>\n",
        "<tr><td>they are               </td> <td>  learning</td></tr>\n",
        "<tr><td>they are learning      </td> <td>  data</td></tr>\n",
        "<tr><td>they are learning data </td> <td>  science</td></tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "_cell_guid": "73254551-40bd-45b1-a7a5-88fe4cbe0b20",
        "_uuid": "ca588b414e70e21bebcead960f6632805d37dd8c",
        "id": "QxcVVARFr3Da",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def generate_padded_sequences(input_sequences):\n",
        "    max_sequence_len = max([len(x) for x in input_sequences])\n",
        "    input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "    \n",
        "    predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
        "    label = ku.to_categorical(label, num_classes=total_words)\n",
        "    return predictors, label, max_sequence_len\n",
        "\n",
        "predictors, label, max_sequence_len = generate_padded_sequences(inp_sequences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "8b5d80ff-54a8-4380-8a3c-149be880551d",
        "_uuid": "8b8a64b96011f427c48d5b0819e3e74af604ce43",
        "id": "gH41QFnAr3Da"
      },
      "source": [
        "Perfect, now we can obtain the input vector X and the label vector Y which can be used for the training purposes. Recent experiments have shown that recurrent neural networks have shown a good performance in sequence to sequence learning and text data applications. Lets look at them in brief.\n",
        "\n",
        "## 4. LSTMs for Text Generation\n",
        "\n",
        "![](http://www.shivambansal.com/blog/text-lstm/2.png)\n",
        "\n",
        "Unlike Feed-forward neural networks in which activation outputs are propagated only in one direction, the activation outputs from neurons propagate in both directions (from inputs to outputs and from outputs to inputs) in Recurrent Neural Networks. This creates loops in the neural network architecture which acts as a â€˜memory stateâ€™ of the neurons. This state allows the neurons an ability to remember what have been learned so far.\n",
        "\n",
        "The memory state in RNNs gives an advantage over traditional neural networks but a problem called Vanishing Gradient is associated with them. In this problem, while learning with a large number of layers, it becomes really hard for the network to learn and tune the parameters of the earlier layers. To address this problem, A new type of RNNs called LSTMs (Long Short Term Memory) Models have been developed.\n",
        "\n",
        "LSTMs have an additional state called â€˜cell stateâ€™ through which the network makes adjustments in the information flow. The advantage of this state is that the model can remember or forget the leanings more selectively. To learn more about LSTMs, here is a great post. Lets architecture a LSTM model in our code. I have added total three layers in the model.\n",
        "\n",
        "1. Input Layer : Takes the sequence of words as input\n",
        "2. LSTM Layer : Computes the output using LSTM units. I have added 100 units in the layer, but this number can be fine tuned later.\n",
        "3. Dropout Layer : A regularisation layer which randomly turns-off the activations of some neurons in the LSTM layer. It helps in preventing over fitting. (Optional Layer)\n",
        "4. Output Layer : Computes the probability of the best possible next word as output\n",
        "\n",
        "We will run this model for total 100 epoochs but it can be experimented further."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "_cell_guid": "60d6721e-e40e-4f2b-8f63-c06459d68f26",
        "_uuid": "76ef6d9352002d333a7c75e8aed7ce996015f527",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6f_QsM92r3Da",
        "outputId": "60316e7c-2c94-4b8f-c192-b643bc43ac27",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 136, 512)          1911808   \n",
            "                                                                 \n",
            " gru (GRU)                   (None, 512)               1575936   \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 512)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 3734)              1915542   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,403,286\n",
            "Trainable params: 5,403,286\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "def create_model(max_sequence_len, total_words):\n",
        "    input_len = max_sequence_len - 1\n",
        "    model = tf.keras.models.Sequential()\n",
        "    \n",
        "    # Add Input Embedding Layer\n",
        "    model.add(tf.keras.layers.Embedding(total_words, 512, input_length=input_len))\n",
        "    \n",
        "    # Add Hidden Layer 1 - LSTM Layer\n",
        "    model.add(tf.keras.layers.GRU(512))\n",
        "    model.add(tf.keras.layers.Dropout(0.5))\n",
        "    \n",
        "    # Add Output Layer\n",
        "    model.add(Dense(total_words, activation='softmax'))\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    \n",
        "    return model\n",
        "\n",
        "model = create_model(max_sequence_len, total_words)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "1826aa1a-cb77-4379-a69d-e9b180945dce",
        "_uuid": "f0b16b471969dbb831cb0024e303341e11b63de4",
        "id": "EYFgN4j-r3Db"
      },
      "source": [
        "Lets train our model now"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "_cell_guid": "07d5cf03-d171-4993-9f8b-18446649ecb0",
        "_uuid": "156f3303b8120cc6932e6db985cbea4a7ceb08bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPGBGLLxr3Db",
        "outputId": "05c049fc-6b58-4b98-daae-9318051c8822",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/35\n",
            "1273/1273 [==============================] - 53s 37ms/step - loss: 5.6852 - accuracy: 0.1224\n",
            "Epoch 2/35\n",
            "1273/1273 [==============================] - 30s 23ms/step - loss: 4.9585 - accuracy: 0.1763\n",
            "Epoch 3/35\n",
            "1273/1273 [==============================] - 29s 23ms/step - loss: 4.4597 - accuracy: 0.2155\n",
            "Epoch 4/35\n",
            "1273/1273 [==============================] - 30s 23ms/step - loss: 3.9254 - accuracy: 0.2624\n",
            "Epoch 5/35\n",
            "1273/1273 [==============================] - 29s 23ms/step - loss: 3.3738 - accuracy: 0.3167\n",
            "Epoch 6/35\n",
            "1273/1273 [==============================] - 29s 22ms/step - loss: 2.8725 - accuracy: 0.3786\n",
            "Epoch 7/35\n",
            "1273/1273 [==============================] - 28s 22ms/step - loss: 2.4248 - accuracy: 0.4487\n",
            "Epoch 8/35\n",
            "1273/1273 [==============================] - 28s 22ms/step - loss: 2.0903 - accuracy: 0.5093\n",
            "Epoch 9/35\n",
            "1273/1273 [==============================] - 28s 22ms/step - loss: 1.8383 - accuracy: 0.5508\n",
            "Epoch 10/35\n",
            "1273/1273 [==============================] - 28s 22ms/step - loss: 1.6680 - accuracy: 0.5867\n",
            "Epoch 11/35\n",
            "1273/1273 [==============================] - 28s 22ms/step - loss: 1.5520 - accuracy: 0.6091\n",
            "Epoch 12/35\n",
            "1273/1273 [==============================] - 28s 22ms/step - loss: 1.4543 - accuracy: 0.6278\n",
            "Epoch 13/35\n",
            "1273/1273 [==============================] - 28s 22ms/step - loss: 1.3726 - accuracy: 0.6433\n",
            "Epoch 14/35\n",
            "1273/1273 [==============================] - 29s 23ms/step - loss: 1.3265 - accuracy: 0.6582\n",
            "Epoch 15/35\n",
            "1273/1273 [==============================] - 28s 22ms/step - loss: 1.2821 - accuracy: 0.6640\n",
            "Epoch 16/35\n",
            "1273/1273 [==============================] - 28s 22ms/step - loss: 1.2394 - accuracy: 0.6724\n",
            "Epoch 17/35\n",
            "1273/1273 [==============================] - 28s 22ms/step - loss: 1.2278 - accuracy: 0.6766\n",
            "Epoch 18/35\n",
            "1273/1273 [==============================] - 28s 22ms/step - loss: 1.2537 - accuracy: 0.6715\n",
            "Epoch 19/35\n",
            "1273/1273 [==============================] - 28s 22ms/step - loss: 1.2282 - accuracy: 0.6772\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe1c030d3c0>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "callback = [EarlyStopping(monitor='loss', patience=5)]\n",
        "model.fit(predictors, label, epochs=35, callbacks=[callback], shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "61e99cfe-7395-4d61-8d1a-8539103d3db5",
        "_uuid": "448bf43b123060dfe4e27cb9f12889e4fe0ed2a7",
        "id": "xMX-GhSOr3Db"
      },
      "source": [
        "## 5. Generating the text \n",
        "\n",
        "Great, our model architecture is now ready and we can train it using our data. Next lets write the function to predict the next word based on the input words (or seed text). We will first tokenize the seed text, pad the sequences and pass into the trained model to get predicted word. The multiple predicted words can be appended together to get predicted sequence.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "_uuid": "e71e56543b7065f115a05e3fd062262b3b94ad46",
        "id": "HVNVD0Uur3Db",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def generate_text(seed_text, model, max_sequence_len):\n",
        "    output_word = \"\"\n",
        "    while output_word != \".\" and output_word != \"!\" and output_word != \"?\":\n",
        "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "        predict_x=model.predict(token_list,verbose=0) \n",
        "        predicted=np.argmax(predict_x, axis=1)\n",
        "        \n",
        "        output_word = \"\"\n",
        "        for word,index in tokenizer.word_index.items():\n",
        "            if index == predicted:\n",
        "                output_word = word\n",
        "                break\n",
        "        seed_text += \" \"+output_word\n",
        "    return seed_text.title()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "_cell_guid": "e38dd280-093b-4091-b82b-9aa90045b107",
        "_kg_hide-input": true,
        "_uuid": "a21548224c9e661a29e3d369e348aada0599bdc9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9SEn47MKr3Dc",
        "outputId": "55aa26cd-edf3-4d01-fcb6-b3b63d25413d",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Is It Okay To Feel Sad? The World Who Will Be An Idea .\n",
            "What Is My Life'S Purpose? Way , I Have No Interest To Do It .\n",
            "What'S The Meaning Of Life? Life .\n",
            "Purpose The Purpose Of Life Is To Be Happy .\n",
            "Love We Have To Stand Up For Something , We Call It A Thief .\n",
            "Grit The Ultimate Measure Of A Man Is Not Where He Stands At Times Of Challenge And Controversy .\n",
            "Sadness You Can'T Do Great Things , You Can Do Better Than The World To Do .\n",
            "Suicide We Have To Stand Up For Those Who Believe In The World Is Made Of Faith .\n",
            "Why Live? I Think You Can Accomplish Anything , Absolutely Anything If You Can Do .\n"
          ]
        }
      ],
      "source": [
        "print (generate_text(\"Is it okay to feel sad?\", model, max_sequence_len))\n",
        "print (generate_text(\"What is my life's purpose?\", model, max_sequence_len))\n",
        "print (generate_text(\"What's the meaning of life?\", model, max_sequence_len))\n",
        "print (generate_text(\"Purpose\", model, max_sequence_len))\n",
        "print (generate_text(\"Love\", model, max_sequence_len))\n",
        "print (generate_text(\"Grit\", model, max_sequence_len))\n",
        "print (generate_text(\"Sadness\", model, max_sequence_len))\n",
        "print (generate_text(\"Suicide\", model, max_sequence_len))\n",
        "print (generate_text(\"Why live?\", model, max_sequence_len))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "b2cfe563-974a-4e05-ad60-233d409d3de1",
        "_uuid": "279f2e20c482b40d707413d0b1842f179a0d3d7b",
        "id": "BQyLHq_Dr3Dc"
      },
      "source": [
        "## Improvement Ideas \n",
        "\n",
        "As we can see, the model has produced the output which looks fairly fine. The results can be improved further with following points:\n",
        "- Adding more data\n",
        "- Fine Tuning the network architecture\n",
        "- Fine Tuning the network parameters\n",
        "\n",
        "Thanks for going through the notebook, please upvote if you liked. "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}