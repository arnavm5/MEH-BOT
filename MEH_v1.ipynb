{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b4da222-d0a4-4cec-89b1-064b96eca0b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-06 17:55:55.248436: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-05-06 17:55:55.398073: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-05-06 17:55:56.019362: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-05-06 17:55:56.024979: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-06 17:55:58.662969: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "466f99e0-072b-46f5-80a7-a59f0c928a47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./dataset/inspiration.csv\", index_col = 0)\n",
    "new_data = data[['Category', 'Quote']]\n",
    "# new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16f7610c-9a43-46f6-9dd9-6d763c0f1dcb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['LOVE', 'LISTENING', 'STEWARDSHIP', 'RESILIENCE', 'EXPLORING',\n",
       "       'KINDNESS', 'HELPING OTHERS', 'OPTIMISM', 'GIVING',\n",
       "       'COMMON GROUND', 'HOPE', 'ACHIEVEMENT', 'GRATITUDE', 'CHARITY',\n",
       "       'PRACTICE', 'GET OUTSIDE', 'FORGIVENESS', 'CHARACTER', 'LAUGHTER',\n",
       "       'HUMILITY', 'ACCEPTANCE', 'DEDICATION', 'BELIEVE IN YOURSELF',\n",
       "       'SMILE', 'PEACE', 'PERSPECTIVE', 'LIVE YOUR DREAMS', 'CONFIDENCE',\n",
       "       'INNOVATION', 'LEARNING', 'UNITY', 'DISCOVERY', 'SELFLESSNESS',\n",
       "       'FITNESS', 'LEADERSHIP', 'FRIENDSHIP', 'CURIOSITY',\n",
       "       'ENCOURAGEMENT', 'SPREAD YOUR WINGS', 'HARD WORK', 'LIVE LIFE',\n",
       "       'JOY', 'WISDOM', 'PERSISTENCE', 'INSPIRATION', 'OVERCOMING',\n",
       "       'CREATIVITY', 'FAMILY', 'CHEER', 'GIVING BACK', 'COMPASSION',\n",
       "       'AMBITION', 'LOYALTY', 'GRIT', 'EDUCATION', 'APPRECIATING NATURE',\n",
       "       'INTEGRITY', 'EMPATHY', 'SELF-CARE', 'TEAMWORK',\n",
       "       'INCLUDING OTHERS', 'COURAGE', 'PERSEVERANCE', 'APPRECIATION',\n",
       "       'MOTIVATION', 'MENTORING', 'HUMOR', 'COMMUNITY', 'BELIEVE',\n",
       "       'IMAGINE', 'PATIENCE', 'MINDFULNESS', 'DETERMINATION', 'BRAVERY',\n",
       "       'PREPARATION', 'PURPOSE', 'CIVILITY', 'MAKING A DIFFERENCE',\n",
       "       'BEING THERE', 'HEALTH', 'COURTESY', 'STRENGTH', 'RESPECT',\n",
       "       'PULL TOGETHER', 'TRUE BEAUTY', 'PASSION', 'EQUALITY', 'HONESTY',\n",
       "       'LITERACY', 'INCLUSION', 'DRIVE', 'WONDER', 'VOLUNTEERING',\n",
       "       'PARENTING', 'SPORTSMANSHIP', 'CARING', 'SOUL', 'SHARING',\n",
       "       'JUSTICE', 'TEACHING BY EXAMPLE', 'RIGHT CHOICES', 'COMPLIMENTS',\n",
       "       'GOOD MANNERS', 'SERVICE', 'RISING ABOVE', 'RESPONSIBILITY',\n",
       "       'DEVOTION', 'TRUST', 'COMMITMENT', 'GENEROSITY', 'OPPORTUNITY',\n",
       "       'DO THY BEST', 'VISION', 'CONNECTION', 'DO YOUR PART',\n",
       "       'REACHING OUT'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data['Category'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffc7c96e-1b3b-40d1-9fb0-01dc4b35399b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file1 = open(\"MyFile.txt\", \"w\")\n",
    "the_length = new_data.shape[0]\n",
    "for i in range(0,the_length):\n",
    "    file1.write(new_data[\"Category\"][i])\n",
    "    file1.write(\": \")\n",
    "    file1.write(new_data[\"Quote\"][i])\n",
    "    file1.write(\"\\n\")\n",
    "    # file1.write(\"\\n\")\n",
    "file1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fff66b34-0d95-41cf-b2cb-737846708349",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing the sentences...\n",
      "Num sentences: 1797\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import LambdaCallback\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.models import Sequential\n",
    "from keras.utils.data_utils import get_file\n",
    "import string\n",
    "\n",
    "print('\\nPreparing the sentences...')\n",
    "max_sentence_len = 40\n",
    "with open(\"MyFile.txt\") as file_:\n",
    "  docs = file_.readlines()\n",
    "sentences = [[word for word in doc.lower().translate(string.punctuation).split()] for doc in docs]\n",
    "# print(sentences)\n",
    "print('Num sentences:', len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0928b722-c945-4d14-b8d0-f8bdb4d6304b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9511/2596606510.py:12: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
      "/tmp/ipykernel_9511/2596606510.py:13: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n"
     ]
    }
   ],
   "source": [
    "text = open(\"MyFile.txt\", \"r\").read()\n",
    "maxlen = 60 #extract sequences of length 60\n",
    "step = 3\n",
    "sentences = []\t#holds extracted sequences\n",
    "next_chars = [] #holds the targets\n",
    "for i in range(0, len(text)-maxlen, step):\n",
    "\tsentences.append(text[i:i+maxlen])\n",
    "\tnext_chars.append(text[i+maxlen])\n",
    "#VECTORIZATION\n",
    "chars = sorted(set(text))\n",
    "char_indices = dict((char, chars.index(char)) for char in chars)\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "\tfor t, char in enumerate(sentence):\n",
    "\t\tx[i, t, char_indices[char]] = 1\n",
    "\ty[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed0623db-0cbf-43fd-a01e-1a96988e5ee8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-06 18:00:56.706794: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-05-06 18:00:56.708393: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-05-06 18:00:56.710707: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(layers.LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(layers.Dense(len(chars), activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1aeada53-a3fe-4336-af94-f9baf03de5db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a630d051-ff8e-45f5-b547-6b5128d78dd3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "\tpreds = np.asarray(preds).astype('float64')\n",
    "\tpreds = np.log(preds) / temperature\n",
    "\texp_preds = np.exp(preds)\n",
    "\tpreds = exp_preds / np.sum(exp_preds)\n",
    "\tprobas = np.random.multinomial(1, preds, 1)\n",
    "\treturn np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "015204ab-625f-4e49-a26f-86c98d367ad3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-06 18:02:32.710695: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-05-06 18:02:32.712937: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-05-06 18:02:32.715449: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-05-06 18:02:33.231586: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-05-06 18:02:33.233498: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-05-06 18:02:33.235568: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "562/562 [==============================] - 50s 87ms/step - loss: 2.9557\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unexpected result of `predict_function` (Empty batch_outputs). Please use `Model.compile(..., run_eagerly=True)`, or `tf.config.run_functions_eagerly(True)` for more information of where went wrong, or file a issue/bug to `tf.keras`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m generated_text \u001b[38;5;241m=\u001b[39m x[start_index: start_index \u001b[38;5;241m+\u001b[39m maxlen]\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m400\u001b[39m): \u001b[38;5;66;03m#generates 400 length string\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerated_text\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      8\u001b[0m     next_index \u001b[38;5;241m=\u001b[39m sample(preds, temperature)\n\u001b[1;32m      9\u001b[0m     next_char \u001b[38;5;241m=\u001b[39m chars[next_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/engine/training.py:2407\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   2403\u001b[0m                 callbacks\u001b[38;5;241m.\u001b[39mon_predict_batch_end(\n\u001b[1;32m   2404\u001b[0m                     end_step, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs\u001b[39m\u001b[38;5;124m\"\u001b[39m: batch_outputs}\n\u001b[1;32m   2405\u001b[0m                 )\n\u001b[1;32m   2406\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m batch_outputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2407\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2408\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected result of `predict_function` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2409\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(Empty batch_outputs). Please use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2410\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`Model.compile(..., run_eagerly=True)`, or \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2411\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.config.run_functions_eagerly(True)` for more \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2412\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minformation of where went wrong, or file a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2413\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124missue/bug to `tf.keras`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2414\u001b[0m         )\n\u001b[1;32m   2415\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_predict_end()\n\u001b[1;32m   2416\u001b[0m all_outputs \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39m__internal__\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mmap_structure_up_to(\n\u001b[1;32m   2417\u001b[0m     batch_outputs, potentially_ragged_concat, outputs\n\u001b[1;32m   2418\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: Unexpected result of `predict_function` (Empty batch_outputs). Please use `Model.compile(..., run_eagerly=True)`, or `tf.config.run_functions_eagerly(True)` for more information of where went wrong, or file a issue/bug to `tf.keras`."
     ]
    }
   ],
   "source": [
    "import random\n",
    "for epoch in range(1, 60):\n",
    "    model.fit(x, y, batch_size=128)\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    generated_text = x[start_index: start_index + maxlen]\n",
    "    for i in range(400): #generates 400 length string\n",
    "        preds = model.predict(generated_text)[0]\n",
    "        next_index = sample(preds, temperature)\n",
    "        next_char = chars[next_index]\n",
    "        generated_text += next_char\n",
    "        generated_text = generated_text[1:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
